{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "This notebook is designed to load financial data, scrape tweets, calculate the sentiment analysis of the tweets, and preprocess data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setup of the cells will be as follows:\n",
    "1. Load modules\n",
    "1. Load and slice the data\n",
    "1. Scrape tweets with snscrape\n",
    "1. Clean tweets\n",
    "1. Load the VADER sentiment analyzer\n",
    "1. Add the compound values to the according dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and slice the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_DOGE = pd.read_csv('../datasets/DOGE-USD.csv')\n",
    "df_DOGE = df_DOGE[1000:]\n",
    "keywords_doge = '(doge%20OR%20dogecoin%20OR%20dogecrypto%20)'\n",
    "\n",
    "df_MONA = pd.read_csv('../datasets/MONA-USD.csv')\n",
    "df_MONA = df_MONA[1000:]\n",
    "keywords_mona = '(mona%20OR%20monacoin%20OR%20monacrypto)'\n",
    "\n",
    "df_SHIB = pd.read_csv('../datasets/SHIB-USD (1).csv')\n",
    "keywords_shib = '(shiba%20OR%20shibacoin%20OR%20shibacrypto%20shiba inu)'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scrape tweets with snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anyOfWords(df, keyword=''):\n",
    "    dates = [x for x in df['Date']]\n",
    "    dates.append('2022-11-17')\n",
    "\n",
    "    tweets_list = []\n",
    "    maxTweets = 100\n",
    "    for x in range(len(dates)):\n",
    "        if x+1 == len(dates):\n",
    "            break\n",
    "        for i, tweet in enumerate(sntwitter.TwitterSearchScraper(f'{keyword} +  since:{dates[x]} until:{dates[x+1]} lang:en' ).get_items()):\n",
    "\n",
    "            if i > maxTweets:\n",
    "                break\n",
    "            else:\n",
    "                tweets_list.append([tweet.date,tweet.content])\n",
    "                print(tweet.date)\n",
    "\n",
    "\n",
    "\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns=['Datetime','Text']) \n",
    "    tweets_df.to_csv('shib_tweets.csv')   \n",
    "\n",
    "\n",
    "    return tweets_list\n",
    "\n",
    "#tweets_doge = anyOfWords(df_DOGE, keywords_doge, 'doge_tweets.csv')\n",
    "#tweets_mona = anyOfWords(df_MONA, keywords_mona)\n",
    "tweets_shib = anyOfWords(df_SHIB, keywords_shib)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_shib = pd.read_csv('shib_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Clean tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0    Datetime  \\\n",
      "0               0  2020-08-01   \n",
      "1               1  2020-08-01   \n",
      "2               2  2020-08-01   \n",
      "3               3  2020-08-01   \n",
      "4               4  2020-08-01   \n",
      "...           ...         ...   \n",
      "82525       82525  2022-11-16   \n",
      "82526       82526  2022-11-16   \n",
      "82527       82527  2022-11-16   \n",
      "82528       82528  2022-11-16   \n",
      "82529       82529  2022-11-16   \n",
      "\n",
      "                                                    Text  \n",
      "0      Home at last! Kuma - Shiba Inu from #Chicago #...  \n",
      "1      me and her/him did a successful trade they wen...  \n",
      "2                                The beige shiba inu won  \n",
      "3       is trusted yey, i gave her 15k for her shiba inu  \n",
      "4      Trying something a little different #Grounded!...  \n",
      "...                                                  ...  \n",
      "82525  Shiba Inu (SHIB) Uncovers Tech Trench HUB For ...  \n",
      "82526  Shiba Inu (SHIB) Uncovers Tech Trench HUB For ...  \n",
      "82527  Shiba Inu (SHIB) Uncovers Tech Trench HUB For ...  \n",
      "82528                 How Shiba Inu compares to Dogecoin  \n",
      "82529  My Shiba Inu has attacked and killed more anim...  \n",
      "\n",
      "[82530 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def tweet_cleaner(text):\n",
    "\n",
    "    #remove RT\n",
    "    text = re.sub(\"RT @[\\w]*:\",\"\",text)\n",
    "    #remove twitter handles (@user)\n",
    "    text = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", text)\n",
    "    #remove url links\n",
    "    text = re.sub(\"https?://[A-Za-z0-9./]*\",\"\",text)\n",
    "    # remove whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "tw_shib['Text'] = tw_shib['Text'].apply(lambda x: tweet_cleaner(x))\n",
    "tw_shib['Datetime'] = pd.to_datetime(tw_shib['Datetime']).dt.date\n",
    "tw_shib.to_csv('shib_tweets_clean.csv')\n",
    "print(tw_shib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Load the VADER sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SentimentIntensityAnalyzer class\n",
    "# from vaderSentiment.vaderSentiment module.\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# function to print sentiments\n",
    "# of the sentence.\n",
    "def sentiment_scores(sentence):\n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # polarity_scores method of SentimentIntensityAnalyzer\n",
    "    # object gives a sentiment dictionary.\n",
    "    # which contains pos, neg, neu, and compound scores.\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "\n",
    "    #print(sentiment_dict)\n",
    "    return sentiment_dict\n",
    "    #print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "    #print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "    #print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "    #print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    " \n",
    "    #print(\"Sentence Overall Rated As\", end = \" \")\n",
    " \n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    #if sentiment_dict['compound'] >= 0.05 :\n",
    "        #print(\"Positive\")\n",
    " \n",
    "    #elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        #print(\"Negative\")\n",
    " \n",
    "    #else :\n",
    "        #print(\"Neutral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Add the compound values to the according dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_compound(df_tweets, df_off, name):\n",
    "    dates = [x for x in df_off['Date']]\n",
    "    list_com_per_day = []\n",
    "\n",
    "    for day in range(len(dates)):\n",
    "        list_com = []\n",
    "        count = 0\n",
    "        for x in range(len(df_tweets)):\n",
    "            if dates[day] == str(df_tweets['Datetime'][x]):\n",
    "                count+=1\n",
    "                d = sentiment_scores(df_tweets['Text'][x])\n",
    "                print(df_tweets['Datetime'][x], d)\n",
    "                list_com.append(d['compound'])\n",
    "        if count != 0:\n",
    "            com_per_day = sum(list_com)/count\n",
    "            list_com_per_day.append(com_per_day)\n",
    "           \n",
    "        else:\n",
    "            list_com_per_day.append('nan')\n",
    "            \n",
    "\n",
    "\n",
    "    list_com_per_day = np.array(list_com_per_day)\n",
    "\n",
    "    df_with_com = df_off\n",
    "    df_with_com['com'] = list_com_per_day\n",
    "    df_with_com.to_csv(f'{name}_com.csv')\n",
    "    return df_with_com\n",
    "\n",
    "\n",
    "tw_shib = pd.read_csv('shib_tweets_clean.csv')\n",
    "add_compound(tw_shib, df_SHIB, 'shib')       \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c3b67288f80e65b9e22ff20ec38e6fe84e43f60819cfa198ebdaec3c4bdb0f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
